# This viminfo file was generated by Vim 9.1.
# You may edit it if you're careful!

# Viminfo version
|1,4

# Value of 'encoding' when this file was written
*encoding=utf-8


# hlsearch on (H) or off (h):
~h
# Last Search Pattern:
~MSle0~/method

# Command Line History (newest to oldest):
:wq!
|2,0,1751551846,,"wq!"
:q!
|2,0,1751551170,,"q!"
:q
|2,0,1751496876,,"q"
:wq1
|2,0,1751386587,,"wq1"
:950
|2,0,1751149172,,"950"
:set nu
|2,0,1751149168,,"set nu"
:Q
|2,0,1751046366,,"Q"

# Search String History (newest to oldest):
?/method
|2,1,1751494842,47,"method"
?/sentiment_counts
|2,1,1751493772,47,"sentiment_counts"
?/total_cate
|2,1,1751493520,47,"total_cate"
?/sentiment counts
|2,1,1751493506,47,"sentiment counts"
?/stream
|2,1,1751492950,47,"stream"
?/senti
|2,1,1751490752,47,"senti"
?/total_
|2,1,1751490617,47,"total_"
?/total_cat
|2,1,1751490611,47,"total_cat"
?/total_records_processed
|2,1,1751461739,47,"total_records_processed"
?/message.max.bytes
|2,1,1751450409,47,"message.max.bytes"
?/message.max
|2,1,1751450397,47,"message.max"
?/message_max
|2,1,1751450388,47,"message_max"
?/socket.request.max.bytes
|2,1,1751450322,47,"socket.request.max.bytes"
?/messages_to_p
|2,1,1751408127,47,"messages_to_p"
?/chunk
|2,1,1751407299,47,"chunk"
?/chunksize
|2,1,1751407140,47,"chunksize"
?/consumer
|2,1,1751406922,47,"consumer"
?/zookeeper.con
|2,1,1751405586,47,"zookeeper.con"
?/listeners
|2,1,1751405383,47,"listeners"
?/batch_size
|2,1,1751403291,47,"batch_size"
?/KafkaServer id
|2,1,1751383186,47,"KafkaServer id"
?/INFO [KafkaServer id=0] started
|2,1,1751382970,47,"INFO [KafkaServer id=0] started"
?/INFO Registered broker 0 with zookeeper
|2,1,1751382959,47,"INFO Registered broker 0 with zookeeper"
?/linger
|2,1,1751368150,47,"linger"
?/batch
|2,1,1751367988,47,"batch"
?/timeouti
|2,1,1751325933,47,"timeouti"
?/Reading
|2,1,1751324611,47,"Reading"
?/hread {os.getpid()}: Reading
|2,1,1751324544,47,"hread {os.getpid()}: Reading"
?/reading
|2,1,1751324500,47,"reading"
?/pid
|2,1,1751324474,47,"pid"
?/read {os.getpid()}: Reading
|2,1,1751324414,47,"read {os.getpid()}: Reading"
?/current_thread
|2,1,1751324305,47,"current_thread"
?/getpid
|2,1,1751324057,47,"getpid"
?/thread.pid
|2,1,1751323990,47,"thread.pid"
?/producer.se
|2,1,1751323729,47,"producer.se"
?/id
|2,1,1751323049,47,"id"
?/nltk
|2,1,1751322803,47,"nltk"
?/offset
|2,1,1751111011,47,"offset"
?/duration_sec
|2,1,1750976843,47,"duration_sec"
?/time.time() - 
|2,1,1750974590,47,"time.time() - "
?/total_reviews
|2,1,1750972148,47,"total_reviews"
?/metric
|2,1,1750972105,47,"metric"
?/BATCH_THREAD_SIZE
|2,1,1750971773,47,"BATCH_THREAD_SIZE"
?/raw_batch
|2,1,1750971736,47,"raw_batch"
?/executor
|2,1,1750971673,47,"executor"
?/features
|2,1,1750971662,47,"features"
?/poll
|2,1,1750971621,47,"poll"
?/THREAD
|2,1,1750971479,47,"THREAD"
?/clear
|2,1,1750968705,47,"clear"

# Expression History (newest to oldest):

# Input Line History (newest to oldest):

# Debug Line History (newest to oldest):

# Registers:
""1	LINE	0
	import streamlit as st
	import pandas as pd
	import plotly.express as px
	from utils import load_summary_dataframe
	from streamlit_autorefresh import st_autorefresh 
	
	# Set page config
	st.set_page_config(page_title="Real-Time IMDB Dashboard", layout="wide")
	st.title(" Real-Time IMDB Sentiment Dashboard")
	
	#  Auto-refresh every 10 seconds
	st_autorefresh(interval=10000, key="auto-refresh")  # interval is in milliseconds
	
	# Load data
	df = load_summary_dataframe()
	
	if df.empty:
	    st.warning("No data available yet.")
	else:
	    # Sliding window: last 5 minutes
	    latest_time = df["timestamp_utc"].max()
	    recent_df = df[df["timestamp_utc"] > latest_time - pd.Timedelta(minutes=5)]
	
	    #  Average Sentiment Over Time
	    fig_sentiment = px.line(recent_df, x="timestamp_utc", y="avg_sentiment_window", title="Average Sentiment Over Time")
	    st.plotly_chart(fig_sentiment, use_container_width=True)
	
	    #  Throughput Over Time
	    fig_throughput = px.line(recent_df, x="timestamp_utc", y="throughput_rec_per_sec", title="Throughput (records/sec)")
	    st.plotly_chart(fig_throughput, use_container_width=True)
	
	    #  Sentiment Count Totals
	    latest_row = df.iloc[-1]
	    sentiment_counts = {
	        "Positive": latest_row["positive_count_total"],
	        "Neutral": latest_row["neutral_count_total"],
	        "Negative": latest_row["negative_count_total"]
	    }
	    st.subheader("Sentiment Count Totals")
	    st.bar_chart(sentiment_counts)
	
	    #  Top 5 Words
	    try:
	        top_words = latest_row["top_words_window"]
	        words_df = pd.DataFrame(top_words, columns=["Word", "Count"])
	        fig_words = px.bar(words_df, x="Word", y="Count", title="Top 5 Words in Sliding Window")
	        st.plotly_chart(fig_words, use_container_width=True)
	    except:
	        st.warning("Top words not available.")
	
|3,1,1,1,50,0,1751551772,"import streamlit as st","import pandas as pd","import plotly.express as px","from utils import load_summary_dataframe","from streamlit_autorefresh import st_autorefresh ","","# Set page config","st.set_page_config(page_title=\"Real-Time IMDB Dashboard\", layout=\"wide\")","st.title(\" Real-Time IMDB Sentiment Dashboard\")","","#  Auto-refresh every 10 seconds",>85
|<"st_autorefresh(interval=10000, key=\"auto-refresh\")  # interval is in milliseconds","","# Load data","df = load_summary_dataframe()","","if df.empty:","    st.warning(\"No data available yet.\")","else:","    # Sliding window: last 5 minutes","    latest_time = df[\"timestamp_utc\"].max()","    recent_df = df[df[\"timestamp_utc\"] > latest_time - pd.Timedelta(minutes=5)]","","    #  Average Sentiment Over Time",>128
|<"    fig_sentiment = px.line(recent_df, x=\"timestamp_utc\", y=\"avg_sentiment_window\", title=\"Average Sentiment Over Time\")","    st.plotly_chart(fig_sentiment, use_container_width=True)","","    #  Throughput Over Time","    fig_throughput = px.line(recent_df, x=\"timestamp_utc\", y=\"throughput_rec_per_sec\", title=\"Throughput (records/sec)\")","    st.plotly_chart(fig_throughput, use_container_width=True)","","    #  Sentiment Count Totals","    latest_row = df.iloc[-1]",>26
|<"    sentiment_counts = {","        \"Positive\": latest_row[\"positive_count_total\"],","        \"Neutral\": latest_row[\"neutral_count_total\"],","        \"Negative\": latest_row[\"negative_count_total\"]","    }","    st.subheader(\"Sentiment Count Totals\")","    st.bar_chart(sentiment_counts)","","    #  Top 5 Words","    try:","        top_words = latest_row[\"top_words_window\"]","        words_df = pd.DataFrame(top_words, columns=[\"Word\", \"Count\"])",>104
|<"        fig_words = px.bar(words_df, x=\"Word\", y=\"Count\", title=\"Top 5 Words in Sliding Window\")","        st.plotly_chart(fig_words, use_container_width=True)","    except:","        st.warning(\"Top words not available.\")",""
"2	LINE	0
	import streamlit as st
	import pandas as pd
	import plotly.express as px
	import time
	from utils import load_summary_dataframe
	
	st.set_page_config(page_title="Real-Time IMDB Dashboard", layout="wide")
	st.title(" Real-Time IMDB Sentiment Dashboard")
	
	REFRESH_INTERVAL = 5  # seconds
	placeholder = st.empty()
	
	with placeholder.container():
	    df = load_summary_dataframe()
	
	    if df.empty:
	        st.warning("No data available yet.")
	    else:
	        # Sliding window: last 5 minutes
	        latest_time = df["timestamp_utc"].max()
	        recent_df = df[df["timestamp_utc"] > latest_time - pd.Timedelta(minutes=5)]
	
	        #  Average Sentiment Over Time
	        fig_sentiment = px.line(recent_df, x="timestamp_utc", y="avg_sentiment_window", title="Average Sentiment Over Time")
	        st.plotly_chart(fig_sentiment, use_container_width=True)
	
	        #  Throughput Over Time
	        fig_throughput = px.line(recent_df, x="timestamp_utc", y="throughput_rec_per_sec", title="Throughput (records/sec)")
	        st.plotly_chart(fig_throughput, use_container_width=True)
	
	        #  Sentiment Counts (latest)
	        latest_row = df.iloc[-1]
	        sentiment_counts = {
	            "Positive": latest_row["positive_count_total"],
	            "Neutral": latest_row["neutral_count_total"],
	            "Negative": latest_row["negative_count_total"]
	        }
	        st.subheader("Sentiment Count Totals")
	        st.bar_chart(sentiment_counts)
	
	        #  Top 5 Words
	        try:
	            top_words = latest_row["top_words_window"]
	            words_df = pd.DataFrame(top_words, columns=["Word", "Count"])
	            fig_words = px.bar(words_df, x="Word", y="Count", title="Top 5 Words in Sliding Window")
	            st.plotly_chart(fig_words, use_container_width=True)
	        except:
	            st.warning("Top words not available.")
	
	        #  Top Movies & Sentiment
|3,0,2,1,50,0,1751551231,"import streamlit as st","import pandas as pd","import plotly.express as px","import time","from utils import load_summary_dataframe","","st.set_page_config(page_title=\"Real-Time IMDB Dashboard\", layout=\"wide\")","st.title(\" Real-Time IMDB Sentiment Dashboard\")","","REFRESH_INTERVAL = 5  # seconds","placeholder = st.empty()","","with placeholder.container():","    df = load_summary_dataframe()","","    if df.empty:",>48
|<"        st.warning(\"No data available yet.\")","    else:","        # Sliding window: last 5 minutes","        latest_time = df[\"timestamp_utc\"].max()","        recent_df = df[df[\"timestamp_utc\"] > latest_time - pd.Timedelta(minutes=5)]","","        #  Average Sentiment Over Time","        fig_sentiment = px.line(recent_df, x=\"timestamp_utc\", y=\"avg_sentiment_window\", title=\"Average Sentiment Over Time\")","        st.plotly_chart(fig_sentiment, use_container_width=True)","",>33
|<"        #  Throughput Over Time","        fig_throughput = px.line(recent_df, x=\"timestamp_utc\", y=\"throughput_rec_per_sec\", title=\"Throughput (records/sec)\")","        st.plotly_chart(fig_throughput, use_container_width=True)","","        #  Sentiment Counts (latest)","        latest_row = df.iloc[-1]","        sentiment_counts = {","            \"Positive\": latest_row[\"positive_count_total\"],","            \"Neutral\": latest_row[\"neutral_count_total\"],",>64
|<"            \"Negative\": latest_row[\"negative_count_total\"]","        }","        st.subheader(\"Sentiment Count Totals\")","        st.bar_chart(sentiment_counts)","","        #  Top 5 Words","        try:","            top_words = latest_row[\"top_words_window\"]","            words_df = pd.DataFrame(top_words, columns=[\"Word\", \"Count\"])","            fig_words = px.bar(words_df, x=\"Word\", y=\"Count\", title=\"Top 5 Words in Sliding Window\")",>66
|<"            st.plotly_chart(fig_words, use_container_width=True)","        except:","            st.warning(\"Top words not available.\")","","        #  Top Movies & Sentiment"
"3	LINE	0
	#  Refresh every 5 seconds
|3,0,3,1,1,0,1751551154,"#  Refresh every 5 seconds"
"4	LINE	0
	time.sleep(REFRESH_INTERVAL)
|3,0,4,1,1,0,1751551153,"time.sleep(REFRESH_INTERVAL)"
"5	LINE	0
	st_autorefresh()
|3,0,5,1,1,0,1751551152,"st_autorefresh()"
"6	LINE	0
	st.rerun()
|3,0,6,1,1,0,1751550993,"st.rerun()"
"7	LINE	0
	st.experimental_rerun()
|3,0,7,1,1,0,1751550714,"st.experimental_rerun()"
"8	LINE	0
	import streamlit as st
	import pandas as pd
	import plotly.express as px
	import time
	import boto3
	import json
	from datetime import datetime
	
	# --- AWS S3 Config ---
	S3_BUCKET = "imdbreviews-scalable"
	S3_PREFIX = "kafka-consumer-outputs/"
	
	# --- Helper functions ---
	def list_summary_keys():
	    s3 = boto3.client('s3')
	    paginator = s3.get_paginator("list_objects_v2")
	    keys = []
	    for page in paginator.paginate(Bucket=S3_BUCKET, Prefix=S3_PREFIX):
	        for obj in page.get('Contents', []):
	            if obj['Key'].endswith('.json'):
	                keys.append(obj['Key'])
	    return sorted(keys)
	
	def read_summary_from_s3(key):
	    s3 = boto3.client('s3')
	    obj = s3.get_object(Bucket=S3_BUCKET, Key=key)
	    return json.loads(obj['Body'].read())
	
	def load_summary_dataframe():
	    keys = list_summary_keys()
	    summaries = [read_summary_from_s3(k) for k in keys]
	    df = pd.DataFrame(summaries)
	    df["timestamp_utc"] = pd.to_datetime(df["timestamp_utc"])
	    df.sort_values("timestamp_utc", inplace=True)
	    return df
	
	# --- Streamlit Dashboard ---
	st.set_page_config(page_title="Real-Time IMDB Dashboard", layout="wide")
	st.title(" Real-Time IMDB Sentiment Dashboard")
	
	# Auto-refresh every 5 seconds
	st.experimental_data_editor({ ["Refreshing every 5 seconds..."]}, use_container_width=True)
	st_autorefresh = st.experimental_rerun  # You can also use st_autorefresh() if using Streamlit >= 1.18
	
	df = load_summary_dataframe()
	
	if df.empty:
	    st.warning("No data available yet.")
	else:
	    # Sliding window: last 5 minutes
|3,0,8,1,50,0,1751548245,"import streamlit as st","import pandas as pd","import plotly.express as px","import time","import boto3","import json","from datetime import datetime","","# --- AWS S3 Config ---","S3_BUCKET = \"imdbreviews-scalable\"","S3_PREFIX = \"kafka-consumer-outputs/\"","","# --- Helper functions ---","def list_summary_keys():","    s3 = boto3.client('s3')","    paginator = s3.get_paginator(\"list_objects_v2\")","    keys = []",>73
|<"    for page in paginator.paginate(Bucket=S3_BUCKET, Prefix=S3_PREFIX):","        for obj in page.get('Contents', []):","            if obj['Key'].endswith('.json'):","                keys.append(obj['Key'])","    return sorted(keys)","","def read_summary_from_s3(key):","    s3 = boto3.client('s3')","    obj = s3.get_object(Bucket=S3_BUCKET, Key=key)","    return json.loads(obj['Body'].read())","","def load_summary_dataframe():","    keys = list_summary_keys()",>57
|<"    summaries = [read_summary_from_s3(k) for k in keys]","    df = pd.DataFrame(summaries)","    df[\"timestamp_utc\"] = pd.to_datetime(df[\"timestamp_utc\"])","    df.sort_values(\"timestamp_utc\", inplace=True)","    return df","","# --- Streamlit Dashboard ---","st.set_page_config(page_title=\"Real-Time IMDB Dashboard\", layout=\"wide\")","st.title(\" Real-Time IMDB Sentiment Dashboard\")","","# Auto-refresh every 5 seconds",>95
|<"st.experimental_data_editor({ [\"Refreshing every 5 seconds...\"]}, use_container_width=True)","st_autorefresh = st.experimental_rerun  # You can also use st_autorefresh() if using Streamlit >= 1.18","","df = load_summary_dataframe()","","if df.empty:","    st.warning(\"No data available yet.\")","else:","    # Sliding window: last 5 minutes"
"9	LINE	0
	import streamlit as st
	import pandas as pd
	import plotly.express as px
	import time
	import boto3
	import json
	from datetime import datetime
	
	# --- AWS S3 Config ---
	S3_BUCKET = "imdbreviews-scalable"
	S3_PREFIX = "kafka-consumer-outputs/"
	
	# --- Helper functions ---
	def list_summary_keys():
	    s3 = boto3.client('s3')
	    paginator = s3.get_paginator("list_objects_v2")
	    keys = []
	    for page in paginator.paginate(Bucket=S3_BUCKET, Prefix=S3_PREFIX):
	        for obj in page.get('Contents', []):
	            if obj['Key'].endswith('.json'):
	                keys.append(obj['Key'])
	    return sorted(keys)
	
	def read_summary_from_s3(key):
	    s3 = boto3.client('s3')
	    obj = s3.get_object(Bucket=S3_BUCKET, Key=key)
	    return json.loads(obj['Body'].read())
	
	def load_summary_dataframe():
	    keys = list_summary_keys()
	    summaries = [read_summary_from_s3(k) for k in keys]
	    df = pd.DataFrame(summaries)
	    df["timestamp_utc"] = pd.to_datetime(df["timestamp_utc"])
	    df.sort_values("timestamp_utc", inplace=True)
	    return df
	
	st.set_page_config(page_title="Real-Time IMDB Dashboard", layout="wide")
	st.title(" Real-Time IMDB Sentiment Dashboard")
	
	REFRESH_INTERVAL = 5  # seconds
	placeholder = st.empty()
	
	while True:
	    with placeholder.container():
	        df = load_summary_dataframe()
	
	        if df.empty:
	            st.warning("No data available yet.")
	            time.sleep(REFRESH_INTERVAL)
	            continue
|3,0,9,1,50,0,1751548080,"import streamlit as st","import pandas as pd","import plotly.express as px","import time","import boto3","import json","from datetime import datetime","","# --- AWS S3 Config ---","S3_BUCKET = \"imdbreviews-scalable\"","S3_PREFIX = \"kafka-consumer-outputs/\"","","# --- Helper functions ---","def list_summary_keys():","    s3 = boto3.client('s3')","    paginator = s3.get_paginator(\"list_objects_v2\")","    keys = []",>73
|<"    for page in paginator.paginate(Bucket=S3_BUCKET, Prefix=S3_PREFIX):","        for obj in page.get('Contents', []):","            if obj['Key'].endswith('.json'):","                keys.append(obj['Key'])","    return sorted(keys)","","def read_summary_from_s3(key):","    s3 = boto3.client('s3')","    obj = s3.get_object(Bucket=S3_BUCKET, Key=key)","    return json.loads(obj['Body'].read())","","def load_summary_dataframe():","    keys = list_summary_keys()",>57
|<"    summaries = [read_summary_from_s3(k) for k in keys]","    df = pd.DataFrame(summaries)","    df[\"timestamp_utc\"] = pd.to_datetime(df[\"timestamp_utc\"])","    df.sort_values(\"timestamp_utc\", inplace=True)","    return df","","st.set_page_config(page_title=\"Real-Time IMDB Dashboard\", layout=\"wide\")","st.title(\" Real-Time IMDB Sentiment Dashboard\")","","REFRESH_INTERVAL = 5  # seconds","placeholder = st.empty()","","while True:","    with placeholder.container():",>39
|<"        df = load_summary_dataframe()","","        if df.empty:","            st.warning(\"No data available yet.\")","            time.sleep(REFRESH_INTERVAL)","            continue"
"-	CHAR	0
	'
|3,0,36,0,1,0,1751326969,"'"

# File marks:
'0  10  9  ~/app.py
|4,48,10,9,1751551846,"~/app.py"
'1  1  23  ~/app.py
|4,49,1,23,1751551411,"~/app.py"
'2  11  23  ~/app.py
|4,50,11,23,1751551411,"~/app.py"
'3  1  21  ~/app.py
|4,51,1,21,1751551274,"~/app.py"
'4  1  21  ~/app.py
|4,52,1,21,1751551274,"~/app.py"
'5  1  21  ~/app.py
|4,53,1,21,1751551274,"~/app.py"
'6  1  21  ~/app.py
|4,54,1,21,1751551274,"~/app.py"
'7  1  0  ~/app.py
|4,55,1,0,1751551170,"~/app.py"
'8  1  0  ~/app.py
|4,56,1,0,1751551170,"~/app.py"
'9  2  0  ~/app.py
|4,57,2,0,1751551170,"~/app.py"

# Jumplist (newest first):
-'  10  9  ~/app.py
|4,39,10,9,1751551846,"~/app.py"
-'  1  0  ~/app.py
|4,39,1,0,1751551772,"~/app.py"
-'  11  23  ~/app.py
|4,39,11,23,1751551411,"~/app.py"
-'  1  21  ~/app.py
|4,39,1,21,1751551392,"~/app.py"
-'  1  21  ~/app.py
|4,39,1,21,1751551274,"~/app.py"
-'  2  0  ~/app.py
|4,39,2,0,1751551170,"~/app.py"
-'  2  0  ~/app.py
|4,39,2,0,1751551170,"~/app.py"
-'  23  0  ~/app.py
|4,39,23,0,1751551149,"~/app.py"
-'  23  0  ~/app.py
|4,39,23,0,1751551149,"~/app.py"
-'  60  15  ~/app.py
|4,39,60,15,1751551113,"~/app.py"
-'  60  15  ~/app.py
|4,39,60,15,1751551113,"~/app.py"
-'  89  15  ~/app.py
|4,39,89,15,1751550999,"~/app.py"
-'  61  15  ~/app.py
|4,39,61,15,1751550999,"~/app.py"
-'  61  15  ~/app.py
|4,39,61,15,1751550999,"~/app.py"
-'  90  9  ~/app.py
|4,39,90,9,1751550990,"~/app.py"
-'  62  9  ~/app.py
|4,39,62,9,1751550990,"~/app.py"
-'  62  9  ~/app.py
|4,39,62,9,1751550990,"~/app.py"
-'  61  9  ~/app.py
|4,39,61,9,1751550717,"~/app.py"
-'  1  9  ~/app.py
|4,39,1,9,1751550710,"~/app.py"
-'  1  9  ~/app.py
|4,39,1,9,1751550710,"~/app.py"
-'  1  9  ~/app.py
|4,39,1,9,1751550710,"~/app.py"
-'  2  9  ~/test.py
|4,39,2,9,1751549072,"~/test.py"
-'  2  9  ~/test.py
|4,39,2,9,1751549072,"~/test.py"
-'  2  9  ~/test.py
|4,39,2,9,1751549072,"~/test.py"
-'  2  9  ~/test.py
|4,39,2,9,1751549072,"~/test.py"
-'  2  9  ~/test.py
|4,39,2,9,1751549072,"~/test.py"
-'  2  9  ~/test.py
|4,39,2,9,1751549072,"~/test.py"
-'  2  9  ~/test.py
|4,39,2,9,1751549072,"~/test.py"
-'  2  9  ~/test.py
|4,39,2,9,1751549072,"~/test.py"
-'  2  9  ~/test.py
|4,39,2,9,1751549072,"~/test.py"
-'  2  9  ~/test.py
|4,39,2,9,1751549072,"~/test.py"
-'  2  9  ~/test.py
|4,39,2,9,1751549072,"~/test.py"
-'  2  9  ~/test.py
|4,39,2,9,1751549072,"~/test.py"
-'  2  9  ~/test.py
|4,39,2,9,1751549072,"~/test.py"
-'  2  9  ~/test.py
|4,39,2,9,1751549072,"~/test.py"
-'  2  9  ~/test.py
|4,39,2,9,1751549072,"~/test.py"
-'  2  9  ~/test.py
|4,39,2,9,1751549072,"~/test.py"
-'  2  9  ~/test.py
|4,39,2,9,1751549072,"~/test.py"
-'  2  9  ~/test.py
|4,39,2,9,1751549072,"~/test.py"
-'  2  9  ~/test.py
|4,39,2,9,1751549072,"~/test.py"
-'  2  9  ~/test.py
|4,39,2,9,1751549072,"~/test.py"
-'  2  9  ~/test.py
|4,39,2,9,1751549072,"~/test.py"
-'  2  9  ~/test.py
|4,39,2,9,1751549072,"~/test.py"
-'  2  9  ~/test.py
|4,39,2,9,1751549072,"~/test.py"
-'  2  9  ~/test.py
|4,39,2,9,1751549072,"~/test.py"
-'  2  9  ~/test.py
|4,39,2,9,1751549072,"~/test.py"
-'  2  9  ~/test.py
|4,39,2,9,1751549072,"~/test.py"
-'  2  9  ~/test.py
|4,39,2,9,1751549072,"~/test.py"
-'  2  9  ~/test.py
|4,39,2,9,1751549072,"~/test.py"
-'  2  9  ~/test.py
|4,39,2,9,1751549072,"~/test.py"
-'  2  9  ~/test.py
|4,39,2,9,1751549072,"~/test.py"
-'  2  9  ~/test.py
|4,39,2,9,1751549072,"~/test.py"
-'  2  9  ~/test.py
|4,39,2,9,1751549072,"~/test.py"
-'  2  9  ~/test.py
|4,39,2,9,1751549072,"~/test.py"
-'  2  9  ~/test.py
|4,39,2,9,1751549072,"~/test.py"
-'  2  9  ~/test.py
|4,39,2,9,1751549072,"~/test.py"
-'  2  9  ~/test.py
|4,39,2,9,1751549072,"~/test.py"
-'  2  9  ~/test.py
|4,39,2,9,1751549072,"~/test.py"
-'  2  9  ~/test.py
|4,39,2,9,1751549072,"~/test.py"
-'  2  9  ~/test.py
|4,39,2,9,1751549072,"~/test.py"
-'  2  9  ~/test.py
|4,39,2,9,1751549072,"~/test.py"
-'  2  9  ~/test.py
|4,39,2,9,1751549072,"~/test.py"
-'  2  9  ~/test.py
|4,39,2,9,1751549072,"~/test.py"
-'  2  9  ~/test.py
|4,39,2,9,1751549072,"~/test.py"
-'  2  9  ~/test.py
|4,39,2,9,1751549072,"~/test.py"
-'  2  9  ~/test.py
|4,39,2,9,1751549072,"~/test.py"
-'  2  9  ~/test.py
|4,39,2,9,1751549072,"~/test.py"
-'  2  9  ~/test.py
|4,39,2,9,1751549072,"~/test.py"
-'  2  9  ~/test.py
|4,39,2,9,1751549072,"~/test.py"
-'  2  9  ~/test.py
|4,39,2,9,1751549072,"~/test.py"
-'  2  9  ~/test.py
|4,39,2,9,1751549072,"~/test.py"
-'  2  9  ~/test.py
|4,39,2,9,1751549072,"~/test.py"
-'  2  9  ~/test.py
|4,39,2,9,1751549072,"~/test.py"
-'  2  9  ~/test.py
|4,39,2,9,1751549072,"~/test.py"
-'  2  9  ~/test.py
|4,39,2,9,1751549072,"~/test.py"
-'  2  9  ~/test.py
|4,39,2,9,1751549072,"~/test.py"
-'  2  9  ~/test.py
|4,39,2,9,1751549072,"~/test.py"
-'  2  9  ~/test.py
|4,39,2,9,1751549072,"~/test.py"
-'  2  9  ~/test.py
|4,39,2,9,1751549072,"~/test.py"
-'  2  9  ~/test.py
|4,39,2,9,1751549072,"~/test.py"
-'  2  9  ~/test.py
|4,39,2,9,1751549072,"~/test.py"
-'  2  9  ~/test.py
|4,39,2,9,1751549072,"~/test.py"
-'  2  9  ~/test.py
|4,39,2,9,1751549072,"~/test.py"
-'  2  9  ~/test.py
|4,39,2,9,1751549072,"~/test.py"
-'  2  9  ~/test.py
|4,39,2,9,1751549072,"~/test.py"
-'  1  0  ~/test.py
|4,39,1,0,1751549060,"~/test.py"
-'  1  0  ~/test.py
|4,39,1,0,1751549060,"~/test.py"
-'  1  0  ~/test.py
|4,39,1,0,1751549060,"~/test.py"
-'  1  0  ~/test.py
|4,39,1,0,1751549060,"~/test.py"
-'  1  0  ~/test.py
|4,39,1,0,1751549060,"~/test.py"
-'  1  0  ~/test.py
|4,39,1,0,1751549060,"~/test.py"
-'  1  0  ~/test.py
|4,39,1,0,1751549060,"~/test.py"
-'  1  0  ~/test.py
|4,39,1,0,1751549060,"~/test.py"
-'  1  0  ~/test.py
|4,39,1,0,1751549060,"~/test.py"
-'  1  0  ~/test.py
|4,39,1,0,1751549060,"~/test.py"
-'  1  0  ~/test.py
|4,39,1,0,1751549060,"~/test.py"
-'  1  0  ~/test.py
|4,39,1,0,1751549060,"~/test.py"
-'  1  0  ~/test.py
|4,39,1,0,1751549060,"~/test.py"
-'  1  0  ~/test.py
|4,39,1,0,1751549060,"~/test.py"
-'  1  0  ~/test.py
|4,39,1,0,1751549060,"~/test.py"

# History of marks within files (newest to oldest):

> ~/app.py
	*	1751551844	0
	"	10	9
	^	10	10
	.	10	10
	+	1	13
	+	1	32
	+	1	10
	+	1	22
	+	1	13
	+	1	2
	+	1	6
	+	1	29
	+	1	22
	+	1	23
	+	1	2
	+	1	10
	+	1	0
	+	1	15
	+	1	47
	+	1	22
	+	1	57
	+	1	6
	+	1	2
	+	1	10
	+	1	49
	+	1	25
	+	1	0
	+	87	27
	+	79	22
	+	61	22
	+	39	22
	+	25	22
	+	10	10

> ~/test.py
	*	1751549070	0
	"	2	9
	^	2	10
	.	2	10
	+	2	10

> ~/utils.py
	*	1751548232	0
	"	1	0
	^	1	0
	.	2	0
	+	29	13
	+	2	0

> ~/visualization.py
	*	1751497026	0
	"	318	49
	^	318	50
	.	318	49
	+	1	9
	+	1	7
	+	1	16
	+	1	7
	+	1	0
	+	1	59
	+	1	7
	+	1	0
	+	1	9
	+	1	39
	+	1	9
	+	1	11
	+	1	9
	+	1	92
	+	1	9
	+	1	46
	+	1	0
	+	1	21
	+	1	0
	+	1	21
	+	1	15
	+	1	11
	+	1	16
	+	1	45
	+	1	91
	+	1	21
	+	1	11
	+	1	7
	+	1	11
	+	1	15
	+	1	11
	+	1	110
	+	1	122
	+	1	11
	+	1	12
	+	1	19
	+	1	18
	+	1	22
	+	1	12
	+	1	0
	+	1	119
	+	1	9
	+	318	49

> ~/imdb_summary.py
	*	1751494849	0
	"	112	27
	^	112	28
	.	112	28
	+	1	23
	+	1	67
	+	1	66
	+	1	0
	+	1	49
	+	1	67
	+	1	0
	+	1	92
	+	1	12
	+	1	23
	+	1	13
	+	1	12
	+	1	23
	+	1	16
	+	1	18
	+	1	10
	+	1	0
	+	1	12
	+	1	18
	+	1	12
	+	1	23
	+	1	12
	+	1	23
	+	1	12
	+	1	23
	+	1	12
	+	146	23
	+	112	28

> ~/.gitignore
	*	1751486929	0
	"	14	11
	^	14	12
	.	14	11
	+	6	41
	+	4	0
	+	7	4
	+	4	0
	+	6	11
	+	8	17
	+	10	7
	+	11	20
	+	12	5
	+	13	13
	+	14	11

> ~/1
	*	1751486477	0
	"	1	0

> ~/imdb_consumer_kafka.py
	*	1751486443	0
	"	267	0
	^	267	10
	.	267	9
	+	1	9
	+	1	10
	+	1	0
	+	1	9
	+	1	10
	+	1	20
	+	1	9
	+	1	10
	+	1	15
	+	1	34
	+	1	38
	+	1	14
	+	1	38
	+	1	64
	+	1	23
	+	1	50
	+	1	9
	+	1	25
	+	1	20
	+	1	12
	+	1	11
	+	1	12
	+	1	9
	+	1	11
	+	1	57
	+	1	63
	+	1	14
	+	1	0
	+	1	9
	+	1	0
	+	1	64
	+	1	9
	+	267	9

> ~/mapreduce_hashtag.py
	*	1751486210	0
	"	12	0
	^	12	31
	.	12	30
	+	1	12
	+	1	11
	+	1	12
	+	1	32
	+	1	21
	+	1	0
	+	1	29
	+	1	19
	+	1	9
	+	1	19
	+	1	0
	+	1	18
	+	1	13
	+	1	18
	+	1	96
	+	1	0
	+	1	12
	+	1	32
	+	1	21
	+	1	59
	+	1	4
	+	1	73
	+	1	22
	+	1	11
	+	1	22
	+	1	11
	+	1	12
	+	1	13
	+	1	14
	+	1	12
	+	1	14
	+	1	13
	+	1	12
	+	1	48
	+	1	54
	+	1	39
	+	1	22
	+	1	6
	+	1	53
	+	1	83
	+	1	34
	+	1	10
	+	1	2
	+	1	16
	+	1	18
	+	1	5
	+	1	12
	+	137	53
	+	12	30

> ~/mapreduce_hashtag
	*	1751486205	0
	"	1	0

> ~/cleaning.py
	*	1751466777	0
	"	36	4
	^	112	47
	.	112	46
	+	112	46

> ~/file_cleaning.py
	*	1751466748	0
	"	129	30
	^	11	31
	.	11	30
	+	1	7
	+	1	90
	+	1	32
	+	1	25
	+	1	12
	+	130	18
	+	6	0
	+	4	10
	+	5	9
	+	4	10
	+	11	30

> ~/producer2.py
	*	1751456949	0
	"	1	0
	^	103	0
	.	102	89
	+	1	12
	+	1	0
	+	1	32
	+	1	12
	+	102	89

> ~/kafka_2.13-3.7.0/config/server.properties
	*	1751450435	0
	"	59	26
	^	59	27
	.	59	26
	+	38	45
	+	34	36
	+	35	45
	+	58	33
	+	59	26

> ~/kafka_2.13-3.7.0/logs/server.log
	*	1751450289	0
	"	1268	0
	^	1268	1

> ~/producer.py
	*	1751448605	0
	"	101	27
	^	101	28
	.	101	27
	+	142	9
	+	14	37
	+	15	29
	+	14	39
	+	12	18
	+	11	35
	+	29	37
	+	124	12
	+	65	27
	+	121	96
	+	122	56
	+	124	0
	+	65	8
	+	101	27

> ~/consumer.py
	*	1751444708	0
	"	1	0
	^	1	0

> ~/imdb_consumer_kafka_new.py
	*	1751408160	0
	"	263	0
	^	169	99
	.	169	99
	+	643	63
	+	585	22
	+	1	21
	+	1	0
	+	1	27
	+	1	0
	+	1	36
	+	1	0
	+	1	9
	+	1	21
	+	1	9
	+	262	10
	+	26	13
	+	20	30
	+	18	31
	+	19	22
	+	128	4
	+	135	0
	+	134	46
	+	129	7
	+	130	7
	+	131	7
	+	132	7
	+	133	7
	+	169	99

> ~/imdb_producer_s3.py
	*	1751407503	0
	"	29	50
	^	29	51
	.	132	96
	+	1	83
	+	1	12
	+	1	94
	+	1	14
	+	1	11
	+	1	12
	+	1	11
	+	1	16
	+	1	1
	+	1	20
	+	1	2
	+	1	12
	+	1	90
	+	1	12
	+	203	0
	+	1	12
	+	1	90
	+	1	29
	+	1	25
	+	1	28
	+	1	16
	+	1	12
	+	1	18
	+	1	19
	+	1	0
	+	1	95
	+	1	44
	+	1	61
	+	1	94
	+	1	15
	+	1	24
	+	1	7
	+	1	77
	+	1	15
	+	1	44
	+	1	45
	+	1	53
	+	1	25
	+	1	0
	+	1	1
	+	1	12
	+	1	14
	+	1	24
	+	1	11
	+	1	12
	+	1	11
	+	1	16
	+	1	20
	+	1	25
	+	1	26
	+	1	32
	+	1	9
	+	1	67
	+	1	18
	+	1	16
	+	1	31
	+	1	27
	+	1	16
	+	1	12
	+	1	93
	+	1	32
	+	1	25
	+	1	20
	+	1	16
	+	1	11
	+	1	12
	+	1	11
	+	1	24
	+	1	12
	+	132	96

> ~/seq_hashtag.py
	*	1751329022	0
	"	159	37
	^	159	38
	.	159	37
	+	1	14
	+	1	12
	+	1	81
	+	1	0
	+	1	12
	+	1	31
	+	1	19
	+	1	0
	+	1	12
	+	159	37

> ~/seq_sentiment.py
	*	1751328480	0
	"	20	19
	^	20	20
	.	20	20
	+	1	14
	+	1	9
	+	1	78
	+	1	0
	+	1	9
	+	224	37
	+	20	20

> ~/seq_wordcount.py
	*	1751327918	0
	"	8	19
	^	8	20
	.	8	19
	+	1	12
	+	1	32
	+	1	0
	+	1	12
	+	139	31
	+	125	13
	+	117	13
	+	108	11
	+	102	11
	+	100	12
	+	98	11
	+	90	14
	+	32	16
	+	35	15
	+	8	19

> ~/mapreduce_sentiment_new.py
	*	1751322803	0
	"	220	4
	^	16	20
	.	16	19
	+	1	84
	+	1	0
	+	1	9
	+	1	84
	+	1	9
	+	220	18
	+	16	19

> ~/mapreduce_hashtag_new.py
	*	1751321179	0
	"	1	0
	.	26	0
	+	213	18
	+	18	35
	+	21	0
	+	26	0

> ~/mapreduce_wordcount_new.py
	*	1751320191	0
	"	41	25
	^	18	26
	.	18	25
	+	227	56
	+	1	8
	+	18	25

> ~/file_split.py
	*	1751319593	0
	"	8	31
	^	8	32
	.	8	31
	+	34	41
	+	8	31

> ~/mapreduce_sentiment.py
	*	1751290861	0
	"	13	24
	^	13	25
	.	13	24
	+	1	12
	+	1	11
	+	1	82
	+	1	21
	+	1	0
	+	1	22
	+	1	0
	+	1	10
	+	1	14
	+	1	11
	+	1	72
	+	1	11
	+	1	12
	+	1	14
	+	1	12
	+	1	14
	+	1	12
	+	1	14
	+	1	12
	+	1	11
	+	1	12
	+	1	11
	+	1	12
	+	1	14
	+	1	12
	+	1	11
	+	1	12
	+	1	11
	+	1	20
	+	1	12
	+	1	14
	+	1	12
	+	1	11
	+	1	12
	+	1	11
	+	1	21
	+	1	12
	+	1	14
	+	1	11
	+	1	16
	+	1	12
	+	1	14
	+	1	12
	+	1	11
	+	1	9
	+	1	12
	+	1	14
	+	1	12
	+	1	9
	+	1	12
	+	1	9
	+	1	12
	+	1	14
	+	1	13
	+	1	12
	+	1	11
	+	1	12
	+	1	9
	+	1	12
	+	1	11
	+	1	12
	+	1	11
	+	1	9
	+	1	12
	+	1	11
	+	1	12
	+	1	11
	+	1	12
	+	1	71
	+	1	9
	+	1	78
	+	1	16
	+	1	44
	+	1	0
	+	1	27
	+	1	34
	+	1	43
	+	1	45
	+	1	23
	+	1	37
	+	1	52
	+	1	0
	+	1	34
	+	1	73
	+	1	55
	+	1	6
	+	1	0
	+	1	34
	+	1	72
	+	1	5
	+	1	90
	+	1	59
	+	1	16
	+	1	18
	+	1	9
	+	130	47
	+	15	30
	+	13	24

> ~/mapreduce_wordcount.py
	*	1751227356	0
	"	133	0
	^	11	40
	.	11	39
	+	1	12
	+	1	33
	+	1	71
	+	1	22
	+	1	0
	+	1	11
	+	1	12
	+	1	14
	+	1	12
	+	1	13
	+	1	12
	+	1	14
	+	1	13
	+	1	11
	+	1	12
	+	1	14
	+	1	12
	+	1	11
	+	1	16
	+	1	12
	+	1	14
	+	1	11
	+	1	16
	+	1	12
	+	1	32
	+	1	12
	+	1	14
	+	1	12
	+	132	63
	+	11	39

> ~/mapreduce_se
	*	1751046366	0
	"	1	0

> ~/mapreduce_Se
	*	1751042850	0
	"	1	0
